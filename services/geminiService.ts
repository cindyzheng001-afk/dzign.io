import { GoogleGenAI, Type } from "@google/genai";
import { FurnitureItem, ColorItem, MiningResponse } from "../types";
import { buildMiningPrompt } from "../constants";

/**
 * Helper to extract mime type and base64 data from a data URL
 */
const getBase64Data = (base64String: string) => {
  const matches = base64String.match(/^data:([a-zA-Z0-9]+\/[a-zA-Z0-9-.+]+);base64,(.+)$/);
  if (matches && matches.length === 3) {
    return { mimeType: matches[1], data: matches[2] };
  }
  return {
    mimeType: 'image/jpeg',
    data: base64String.replace(/^data:image\/[a-z]+;base64,/, "")
  };
};

/**
 * Compresses and resizes an image to ensure it fits within API payload limits.
 * Increased max dimension to 1024px to preserve architectural details.
 */
const compressImage = async (base64String: string): Promise<string> => {
  return new Promise((resolve, reject) => {
    const img = new Image();
    img.onload = () => {
      const canvas = document.createElement('canvas');
      let width = img.width;
      let height = img.height;
      const maxDim = 1024; // Increased from 512 to 1024 for better structural detail

      if (width > maxDim || height > maxDim) {
        if (width > height) {
          height = Math.round((height * maxDim) / width);
          width = maxDim;
        } else {
          width = Math.round((width * maxDim) / height);
          height = maxDim;
        }
      }

      canvas.width = width;
      canvas.height = height;
      const ctx = canvas.getContext('2d');
      if (!ctx) {
        reject(new Error("Could not get canvas context"));
        return;
      }
      
      // Use high quality interpolation
      ctx.imageSmoothingEnabled = true;
      ctx.imageSmoothingQuality = 'high';
      
      ctx.drawImage(img, 0, 0, width, height);
      // Convert to JPEG with 0.8 quality for better detail retention
      resolve(canvas.toDataURL('image/jpeg', 0.8));
    };
    img.onerror = (err) => reject(err);
    img.src = base64String;
  });
};

export const restyleRoom = async (base64Image: string, prompt: string): Promise<string> => {
  try {
    const apiKey = process.env.API_KEY;
    if (!apiKey || apiKey.trim() === '') {
       throw new Error("Service configuration error.");
    }
    
    const client = new GoogleGenAI({ apiKey });

    // Optimize image before sending
    const optimizedImage = await compressImage(base64Image);
    const { mimeType, data } = getBase64Data(optimizedImage);
    
    const response = await client.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: {
        parts: [
          { inlineData: { data, mimeType } },
          { text: prompt },
        ],
      },
      config: {
        temperature: 0.2, // Low temperature to force strict adherence to structural constraints
      }
    });

    // Check for generated images in the response
    if (response.candidates?.[0]?.content?.parts) {
      for (const part of response.candidates[0].content.parts) {
        if (part.inlineData && part.inlineData.data) {
          return `data:image/png;base64,${part.inlineData.data}`;
        }
      }
    }

    // Check if the model refused to generate due to safety/filters
    if (response.candidates?.[0]?.finishReason) {
        console.warn("Model Finish Reason:", response.candidates[0].finishReason);
        throw new Error(`The AI couldn't generate the image (Reason: ${response.candidates[0].finishReason}). Please try a different photo or style.`);
    }

    throw new Error("No image generated by the AI. Please try again.");
  } catch (e) {
    console.error("Restyle Room Error:", e);
    throw e;
  }
};

export const mineFurnitureData = async (base64Image: string, focusItems?: string): Promise<MiningResponse> => {
  try {
    const apiKey = process.env.API_KEY;
    if (!apiKey || apiKey.trim() === '') {
       throw new Error("Service configuration error.");
    }
    
    const client = new GoogleGenAI({ apiKey });
    const optimizedImage = await compressImage(base64Image);
    const { mimeType, data } = getBase64Data(optimizedImage);
    
    const prompt = buildMiningPrompt(focusItems);

    // Use gemini-2.5-flash for analyzing the image (Multimodal Vision)
    const response = await client.models.generateContent({
      model: 'gemini-2.5-flash', 
      contents: {
        parts: [
          { inlineData: { data, mimeType } },
          { text: prompt }
        ]
      },
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            furniture: {
              type: Type.ARRAY,
              items: {
                type: Type.OBJECT,
                properties: {
                  itemName: { type: Type.STRING },
                  color: { type: Type.STRING },
                  searchQuery: { type: Type.STRING },
                },
                required: ["itemName", "color", "searchQuery"],
              },
            },
            palette: {
              type: Type.ARRAY,
              items: {
                type: Type.OBJECT,
                properties: {
                  hex: { type: Type.STRING },
                  name: { type: Type.STRING },
                },
                required: ["hex", "name"],
              }
            }
          },
          required: ["furniture", "palette"]
        },
      }
    });

    if (response.text) {
      return JSON.parse(response.text) as MiningResponse;
    }
    return { furniture: [], palette: [] };
  } catch (e) {
    console.error("Mining Data Error:", e);
    return { furniture: [], palette: [] };
  }
};